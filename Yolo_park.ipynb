{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo_park.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/APA417/2024WinterHoliday/blob/main/Yolo_park.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LDWYg0DGvHqN"
      },
      "cell_type": "code",
      "source": [
        "'''Detecting number of occupied parking spots. Code written keeping in mind the resources provided\n",
        "   under google colab\n",
        "'''\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNRzAVNy-izy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "547872a8-f9f6-4627-a907-0a7fd6772f6d"
      },
      "cell_type": "code",
      "source": [
        "#mount your google drive to avoid losing data on colab when session expires\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-819350928.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#mount your google drive to avoid losing data on colab when session expires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYbwnOdy4F8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXOJ-sOyiSqs"
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/AK49/Detecting_occupied_spaces\")\n",
        "# Directory of images to run detection on\n",
        "ROOT_DIR = os.getcwd()\n",
        "print(ROOT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11sRDwtbiPBT"
      },
      "cell_type": "code",
      "source": [
        "!pip install Cython\n",
        "!git clone https://github.com/waleedka/coco\n",
        "!pip install -U setuptools\n",
        "!pip install -U wheel\n",
        "\n",
        "os.getcwd()\n",
        "!mkdir ak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLVvkwHlsiRu"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2\n",
        "from util import *\n",
        "from darknet import Darknet\n",
        "from preprocess import prep_image, inp_to_image, letterbox_image\n",
        "import pandas as pd\n",
        "import random\n",
        "import pickle as pkl\n",
        "import argparse\n",
        "\n",
        "\n",
        "def get_test_input(input_dim, CUDA):\n",
        "    img = cv2.imread(\"Khare_frame_02.png\")\n",
        "    img = cv2.resize(img, (input_dim, input_dim))\n",
        "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
        "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
        "    img_ = torch.from_numpy(img_).float()\n",
        "    img_ = Variable(img_)\n",
        "\n",
        "    if CUDA:\n",
        "        img_ = img_.cuda()\n",
        "\n",
        "    return img_\n",
        "\n",
        "def prep_image(img, inp_dim):\n",
        "    \"\"\"\n",
        "    Prepare image for inputting to the neural network.\n",
        "\n",
        "    Returns a Variable\n",
        "    \"\"\"\n",
        "\n",
        "    orig_im = img\n",
        "    dim = orig_im.shape[1], orig_im.shape[0]\n",
        "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
        "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
        "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
        "    return img_, orig_im, dim\n",
        "\n",
        "def write(x, img, classes):\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    cls = int(x[-1])\n",
        "    label = \"{0}\".format(classes[cls])\n",
        "    color = (0,0,255)\n",
        "    cv2.rectangle(img, c1, c2,color, 1)\n",
        "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
        "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "    cv2.rectangle(img, c1, c2,color, -1)\n",
        "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
        "    return img\n",
        "\n",
        "def arg_parse():\n",
        "    \"\"\"\n",
        "    Parse arguements to the detect module\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
        "\n",
        "    parser.add_argument(\"--video\", dest = 'video', help =\n",
        "                        \"Video to run detection upon\",\n",
        "                        default = \"ff.mp4\", type = str)\n",
        "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
        "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
        "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
        "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help =\n",
        "                        \"Config file\",\n",
        "                        default = \"./cfg/yolov3.cfg\", type = str)\n",
        "    parser.add_argument(\"--weights\", dest = 'weightsfile', help =\n",
        "                        \"weightsfile\",\n",
        "                        default = \"yolov3.weights\", type = str)\n",
        "    parser.add_argument(\"--reso\", dest = 'reso', help =\n",
        "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
        "                        default = \"704\", type = str)\n",
        "\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6zWdYhCZzQAB"
      },
      "cell_type": "code",
      "source": [
        "def yolo_videoprocess():\n",
        "    batch_size = 15\n",
        "    args = arg_parse()\n",
        "    confidence = float(args.confidence)\n",
        "    nms_thesh = float(args.nms_thresh)\n",
        "    start = 0\n",
        "\n",
        "    CUDA = torch.cuda.is_available()\n",
        "\n",
        "    num_classes = 1\n",
        "\n",
        "    CUDA = torch.cuda.is_available()\n",
        "\n",
        "    bbox_attrs = 5 + num_classes\n",
        "\n",
        "    print(\"Loading network.....\")\n",
        "    model = Darknet(args.cfgfile)\n",
        "    model.load_weights(args.weightsfile)\n",
        "    print(\"Network successfully loaded\")\n",
        "\n",
        "    model.net_info[\"height\"] = args.reso\n",
        "    inp_dim = int(model.net_info[\"height\"])\n",
        "    #uncomment this if error occurs due to frame size\n",
        "#     assert inp_dim % 32 == 0\n",
        "#     assert inp_dim > 32\n",
        "\n",
        "    if CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    model(get_test_input(inp_dim, CUDA), CUDA)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    videofile = args.video\n",
        "\n",
        "    cap = cv2.VideoCapture(videofile)\n",
        "\n",
        "    assert cap.isOpened(), 'Cannot capture source'\n",
        "    frame_count = 0\n",
        "    frames = []\n",
        "    start = time.time()\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_count += 1\n",
        "            frames.append(frame)\n",
        "#             print(\"appended\")\n",
        "            if len(frames) == batch_size:\n",
        "              for i, item in enumerate(frames):\n",
        "                frame = item\n",
        "                img, orig_im, dim = prep_image(frame, inp_dim)\n",
        "\n",
        "                im_dim = torch.FloatTensor(dim).repeat(1,2)\n",
        "\n",
        "\n",
        "                if CUDA:\n",
        "                    im_dim = im_dim.cuda()\n",
        "                    img = img.cuda()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output = model(Variable(img), CUDA)\n",
        "                output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
        "                print(\"count cars {}\".format(output.size(0)))\n",
        "\n",
        "                if type(output) == int:\n",
        "                    frame_count+=1\n",
        "                    print(\"FPS of the video is {:5.2f}\".format( frame_count / (time.time() - start)))\n",
        "                    cv2.imshow(\"frame\", orig_im)\n",
        "                    key = cv2.waitKey(1)\n",
        "                    if key & 0xFF == ord('q'):\n",
        "                        break\n",
        "                    continue\n",
        "\n",
        "\n",
        "                im_dim = im_dim.repeat(output.size(0), 1)\n",
        "                scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
        "\n",
        "                output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
        "                output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
        "\n",
        "                output[:,1:5] /= scaling_factor\n",
        "\n",
        "                for i in range(output.shape[0]):\n",
        "                    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
        "                    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
        "\n",
        "                classes = load_classes('data/coco.names')\n",
        "                colors = pkl.load(open(\"pallete\", \"rb\"))\n",
        "\n",
        "                list(map(lambda x: write(x, orig_im, classes), output))\n",
        "                empty = 6 - output.size(0)\n",
        "                cv2.putText(orig_im, \"Total empty spots: \" + str(empty), (5,30), cv2.FONT_HERSHEY_SIMPLEX, 2, [255,255,255], 9, cv2.LINE_AA)\n",
        "\n",
        "    #             cv2.imshow(\"Parking 1\", orig_im)\n",
        "    #             frames += 1\n",
        "                name = '{0}.jpg'.format(frame_count + i - batch_size)\n",
        "\n",
        "                name = os.path.join('./ak', name)\n",
        "                cv2.imwrite(name, frame)\n",
        "\n",
        "\n",
        "                key = cv2.waitKey(1)\n",
        "                if key & 0xFF == ord('q'):\n",
        "                    break\n",
        "              print(\"FPS of the video is {:5.2f}\".format( frame_count / (time.time() - start)))\n",
        "\n",
        "              frames = []\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    images = list(glob.iglob(os.path.join('./ak', '*.*')))\n",
        "    images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "    make_video('./out.mp4', images, fps=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ToeBIlEdPwY"
      },
      "cell_type": "code",
      "source": [
        "args = arg_parse()\n",
        "videofile = args.video\n",
        "video = cv2.VideoCapture(videofile)\n",
        "\n",
        "# Find OpenCV version\n",
        "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
        "\n",
        "if int(major_ver)  < 3 :\n",
        "    fps = video.get(cv2.cv.CV_CAP_PROP_FPS)\n",
        "    print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
        "else :\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "\n",
        "video.release();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-ndlFIy7bXk"
      },
      "cell_type": "code",
      "source": [
        "# Get all image file paths to a list.\n",
        "# Sort the images by name index.\n",
        "# images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "def make_video(outvid, images=None, fps=25, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        "\n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*format)\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3Z0FbJ_zuOo"
      },
      "cell_type": "code",
      "source": [
        "yolo_videoprocess() #main program for detecting number of cars in the frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gsUQlMrCHOWD"
      },
      "cell_type": "code",
      "source": [
        "!rm -r ak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXVPdrk9qsf0"
      },
      "cell_type": "code",
      "source": [
        "!mkdir ak #don't execute remove and make dir in the same cell on colab, it creates error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0dwZOANvuT22"
      },
      "cell_type": "code",
      "source": [
        "#optional code to create video again with different fps\n",
        "images = list(glob.iglob(os.path.join('./ak/', '*.*')))\n",
        "images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "make_video('out1.mp4', images, fps=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T-PFuE8SJkAY"
      },
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b3jsrBWnhXOQ"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}